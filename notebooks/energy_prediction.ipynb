{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55d23ba",
   "metadata": {},
   "source": [
    "# Problem Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390bef4",
   "metadata": {},
   "source": [
    "## Dataset Description & Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac42fd",
   "metadata": {},
   "source": [
    "### Appliances Energy Prediction Dataset\n",
    "\n",
    "The **Appliances Energy Prediction Dataset** consists of **19,735 time-series measurements**, collected at **10-minute intervals** over approximately **4.5 months** in a low-energy smart home. Each observation includes **28 features** derived from environmental sensors, energy meters, and external weather stations.\n",
    "\n",
    "#### 📌 Features Overview\n",
    "\n",
    "- **Indoor conditions (continuous):**  \n",
    "  - **Temperatures (°C):** `T1` to `T9` (e.g., kitchen, living room, bathroom, bedrooms, outside north wall, etc.)  \n",
    "  - **Humidity (%):** `RH_1` to `RH_9` (same locations as temperature sensors)  \n",
    "\n",
    "- **Weather station variables (continuous):**  \n",
    "  - `T_out` (outdoor temperature), `RH_out` (outdoor humidity), `Press_mm_hg` (pressure),  \n",
    "    `Windspeed`, `Visibility`, `Tdewpoint` (dew point temperature)\n",
    "\n",
    "- **Other numerical variables (continuous):**  \n",
    "  - `lights` (energy usage in Wh), `rv1`, `rv2` (non-dimensional random variables)\n",
    "\n",
    "- **Target variable (continuous):**  \n",
    "  - `Appliances` (energy consumption in Wh)\n",
    "\n",
    "#### 🛠️ Data Characteristics\n",
    "\n",
    "- No missing values\n",
    "- Measurements collected via ZigBee sensors and M‑Bus energy meters\n",
    "- Weather data merged from Chievres Airport (Belgium)\n",
    "- Dataset donated in February 2017\n",
    "- Introduced and analyzed in a study by Candanedo et al. (2017)\n",
    "- Commonly used for regression-based energy forecasting and time-series modeling\n",
    "\n",
    "**Source:** [UCI Machine Learning Repository - Appliances Energy Prediction](https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ecac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from scipy.signal import spectrogram\n",
    "from models.predict_model import eval_model\n",
    "import random\n",
    "from data.make_dataset import load_dataset\n",
    "\n",
    "from models.train_model import fit_model\n",
    "from models.lstm import LSTMForecaster\n",
    "from models.transformer import TransfForecaster\n",
    "from models.tcn import TCNForecaster\n",
    "\n",
    "# Fix Random Seed\n",
    "RANDOM_SEED = 18\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# For CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# For MPS (Apple Silicon) or general determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f1781",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4be00",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20b62e",
   "metadata": {},
   "source": [
    "Checking for missing or NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.columns\n",
    "\n",
    "# Check for missing values or NaNs\n",
    "missing_values = df.isnull().sum()\n",
    "nan_values = df.isna().sum()\n",
    "\n",
    "print(\"Missing values per column:\\n\", missing_values[missing_values > 0])\n",
    "print(\"\\nNaN values per column:\\n\", nan_values[nan_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef8af8",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c10ea",
   "metadata": {},
   "source": [
    "### 📉 Sample Feature Trends (First Week)\n",
    "\n",
    "This panel shows the **temporal dynamics** of key features. Indoor variables (e.g., `T1`, `RH_1`, `lights`) reflect daily patterns and user activity, while outdoor variables (e.g., `T_out`, `Windspeed`, `Pressure`) show natural environmental changes. The target (`Appliances`) displays **bursty usage patterns**, supporting the need for time-aware models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207632f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first week\n",
    "df_subset = df.iloc[:144*7]\n",
    "\n",
    "# Select features to plot\n",
    "selected_features = {\n",
    "    \"Temperature (T1)\": \"T1\",\n",
    "    \"Humidity (RH_1)\": \"RH_1\",\n",
    "    \"Outdoor Temp (T_out)\": \"T_out\",\n",
    "    \"Lights\": \"lights\",\n",
    "    \"Pressure\": \"Press_mm_hg\",\n",
    "    \"Windspeed\": \"Windspeed\",\n",
    "    \"Visibility\": \"Visibility\",\n",
    "    \"Dew Point (Tdewpoint)\": \"Tdewpoint\",\n",
    "    \"Random (rv1)\": \"rv1\",\n",
    "    \"Target (Appliances)\": \"Appliances\"\n",
    "}\n",
    "\n",
    "# Plot 3x4 grid\n",
    "fig, axes = plt.subplots(5, 2, figsize=(18, 10), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (label, col) in enumerate(selected_features.items()):\n",
    "    axes[i].plot(df_subset.index, df_subset[col])\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(selected_features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle(\"Representative Features - First 1000 Timestamps (10-min resolution)\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5d0a",
   "metadata": {},
   "source": [
    "### 🔌 Appliance Energy Usage Distribution\n",
    "\n",
    "The target variable `Appliances` shows a **right-skewed distribution**, with most energy usage values below **200 Wh** and occasional high peaks above **1000 Wh**. This suggests infrequent but significant bursts in consumption, likely during periods of high appliance usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ea0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(df[\"Appliances\"], kde=True)\n",
    "plt.title('Distribution of Appliance Energy Usage')\n",
    "plt.xlabel('Energy Usage (Wh)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306d13d",
   "metadata": {},
   "source": [
    "### Feature Relationship Analysis\n",
    "\n",
    "We analyzed the relationships among variables in the **Appliances Energy Prediction** dataset using a **correlation matrix**:\n",
    "\n",
    "**Correlation Matrix** (linear dependencies):\n",
    "- Strong positive correlations are observed among indoor temperature sensors (`T1` to `T9`), reflecting similar thermal trends across different rooms.\n",
    "- Relative humidity sensors (`RH_1` to `RH_9`) also show moderate correlations, especially among those located in the same area.\n",
    "- The `Appliances` variable shows weak linear correlations with most features, suggesting that energy consumption may be driven by **nonlinear relationships** or **external factors** not captured by simple correlation.\n",
    "- As expected, `rv1` and `rv2` are **perfectly correlated** with each other, but **uncorrelated** with the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b24fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Sort correlation matrix by alphabetical order of columns\n",
    "sorted_cols = sorted(corr.columns)\n",
    "corr_sorted = corr.loc[sorted_cols, sorted_cols]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_sorted, cmap='coolwarm', annot=False, center=0)\n",
    "plt.title('Correlation Matrix (Alphabetically Ordered)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2fadb",
   "metadata": {},
   "source": [
    "### Temporal Structure of Appliance Energy Consumption\n",
    "\n",
    "The autocorrelation plot of the target variable (`Appliances`) reveal key temporal dependencies:\n",
    "\n",
    "- **Autocorrelation (ACF)** shows strong positive correlations at short lags (1–10), indicating that recent past values are highly predictive of current consumption.\n",
    "- A clear **cyclical pattern emerges around lag 144**, which corresponds to 24 hours of data at 10-minute intervals. This suggests a strong **daily seasonality** in appliance usage, likely driven by human activity patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(df[\"Appliances\"].squeeze(), lags=200)\n",
    "plt.title(\"Autocorrelation of Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2517220",
   "metadata": {},
   "source": [
    "## Non-Stationary Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7dc36",
   "metadata": {},
   "source": [
    "### Visual Methods\n",
    "\n",
    "A visual inspection of the time series through multiple analytical perspectives reveals clear signs of **non-stationarity**:\n",
    "\n",
    "- **Time Series Plot**: The raw time series shows large variability in amplitude over time and visually distinct daily peaks, suggesting non-constant mean and variance.\n",
    "\n",
    "- **Rolling Statistics**: The rolling mean and standard deviation fluctuate considerably across time, further indicating that the statistical properties of the series are not stable.\n",
    "\n",
    "- **Seasonal Decomposition**: The decomposition reveals a strong and regular **seasonal component** (daily cycles) and a non-constant **trend**, both of which violate the assumption of stationarity.\n",
    "\n",
    "- **Autocorrelation (ACF)**: The ACF decays slowly and exhibits periodic spikes, suggesting long-term dependencies and seasonality in the data, common in non-stationary series.\n",
    "\n",
    "- **FFT and Spectrogram (STFT)**: Both frequency-domain analyses highlight dominant frequencies corresponding to **daily and sub-daily periodicities**, which are typical of non-stationary signals with repetitive behavior over time.\n",
    "\n",
    "**Conclusion**: The energy consumption series shows **time-varying mean**, **seasonality**, and **long-term structure**, confirming it is **not stationary**. This justifies the need for transformations (e.g., differencing, seasonal adjustment, log transforms) before applying models that assume stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time window (10-minute resolution)\n",
    "window = 6\n",
    "\n",
    "# Compute rolling statistics\n",
    "roll_mean = df[\"Appliances\"].rolling(window=window).mean()\n",
    "roll_std = df[\"Appliances\"].rolling(window=window).std()\n",
    "\n",
    "# Select first 7 days (7*144 = 1008 samples)\n",
    "end_date = df.index[0] + pd.Timedelta(days=7)\n",
    "signal_7d = df.loc[df.index < end_date, \"Appliances\"]\n",
    "roll_mean_7d = roll_mean.loc[signal_7d.index]\n",
    "roll_std_7d = roll_std.loc[signal_7d.index]\n",
    "\n",
    "# Plot rolling statistics\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(signal_7d, label='Original', alpha=0.4)\n",
    "plt.plot(roll_mean_7d, label='1-day Rolling Mean', color='blue')\n",
    "plt.plot(roll_std_7d, label='1-day Rolling Std', color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"1-Day Rolling Mean and Std (First 7 Days)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Appliance Energy Usage (Wh)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb842e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram\n",
    "fs = 1.0  # 1 sample per step (10 minutes per sample)\n",
    "f, t, Sxx = spectrogram(np.ravel(df[\"Appliances\"]), fs=fs, nperseg=256, noverlap=64)\n",
    "\n",
    "# Map t to timestamps (each unit in t is one 10-minute step)\n",
    "timestamps = df.index[0] + pd.to_timedelta(t * 10, unit='min')\n",
    "\n",
    "# Plot spectrogram with time labels\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.pcolormesh(timestamps, f, Sxx, shading='gouraud')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time')\n",
    "plt.title('Spectrogram (STFT)')\n",
    "plt.colorbar(label='Power')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15bf06",
   "metadata": {},
   "source": [
    "## Quantitative Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee65ab3",
   "metadata": {},
   "source": [
    "### 📊 ADF Test Comparison — Global vs. Local Stationarity\n",
    "\n",
    "We performed two types of Augmented Dickey-Fuller (ADF) tests to assess the stationarity of the `Appliances` energy consumption time series:\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧪 1. Standard (Global) ADF Test\n",
    "\n",
    "- **ADF Statistic:** –21.62  \n",
    "- **p-value:** 0.0000  \n",
    "- ✅ **Conclusion:** Rejects the null hypothesis of non-stationarity → the series is **globally stationary**.\n",
    "\n",
    "However, this result may be **misleading**, as the signal clearly exhibits **daily seasonality and trend components**, as shown in the seasonal decomposition and FFT analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 2. Rolling ADF Test (Local, 4-Hour Window)\n",
    "\n",
    "- Applied ADF in a moving window over the **first 10 days** using a **4-hour window** stepped every 100 minutes.\n",
    "- The **p-value fluctuates** significantly, frequently **crossing above and below the 0.05 threshold**.\n",
    "- This suggests **intermittent (local) stationarity** and temporal variation in the statistical properties of the signal.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Key Takeaways\n",
    "\n",
    "- The **global ADF test falsely suggests full stationarity**, likely due to strong seasonal patterns.\n",
    "- The **rolling ADF test provides a more nuanced view**, revealing that the signal is **non-stationary in many local segments**, especially during volatile consumption periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ad065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run ADF test on the target variable\n",
    "adf_result = adfuller(df[\"Appliances\"])\n",
    "adf_statistic, p_value, lags, n_obs, crit_values, ic_best = adf_result\n",
    "\n",
    "print(f\"ADF Statistic: {adf_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4g}\")\n",
    "print(f\"# Lags Used: {lags}\")\n",
    "print(f\"# Observations: {n_obs}\")\n",
    "print(\"Critical Values:\")\n",
    "for k, v in crit_values.items():\n",
    "    print(f\"   {k}: {v:.4f}\")\n",
    "print(f\"Best Information Criterion (IC): {ic_best:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Define parameters\n",
    "samples_per_day = 144  # 10-min intervals\n",
    "window_size = 24  # 4 hours\n",
    "step = 10  # slide by 10 time steps = 100 minutes\n",
    "max_samples = samples_per_day * 10  # First 10 days = 1440 samples\n",
    "\n",
    "# Store p-values and timestamps\n",
    "pvals = []\n",
    "stats = []\n",
    "timestamps = []\n",
    "\n",
    "# Rolling ADF loop only for the first 10 days\n",
    "for start in range(0, max_samples - window_size, step):\n",
    "    end = start + window_size\n",
    "    segment = df[\"Appliances\"].iloc[start:end]\n",
    "    \n",
    "    try:\n",
    "        stat, p, *_ = adfuller(segment)\n",
    "        pvals.append(p)\n",
    "        stats.append(stat)\n",
    "        timestamps.append(df.index[start + window_size // 2])\n",
    "    except:\n",
    "        pvals.append(np.nan)\n",
    "        stats.append(np.nan)\n",
    "        timestamps.append(df.index[start + window_size // 2])\n",
    "\n",
    "# Plot p-values\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(timestamps, pvals, label=\"ADF p-value\", color='darkorange')\n",
    "plt.axhline(0.05, color='red', linestyle='--', label='Significance Level (0.05)')\n",
    "plt.title(\"Rolling ADF Test (4 Hours, First 10 Days)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c533b",
   "metadata": {},
   "source": [
    "## Seasonal Decomposition Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f0f62",
   "metadata": {},
   "source": [
    "### 🔁 Frequency Analysis with FFT\n",
    "\n",
    "We applied the **Fast Fourier Transform (FFT)** to the `Appliances` time series (after mean-centering) to identify dominant periodic patterns. The frequency spectrum reveals a clear peak at a non-zero frequency, corresponding to a **season length of 145 samples**, which closely aligns with **1 day** (144 samples at 10-minute intervals).\n",
    "\n",
    "This confirms the presence of a strong **daily consumption cycle**, consistent with expected household energy usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "import numpy as np\n",
    "\n",
    "n = len(df[\"Appliances\"])\n",
    "y_fft = fft(df[\"Appliances\"] - np.mean(df[\"Appliances\"]))\n",
    "freqs = np.fft.fftfreq(n)\n",
    "peak_freq = abs(freqs[np.argmax(np.abs(y_fft[1:n//2]))])  # avoid 0 frequency\n",
    "season_length = int(round(1 / peak_freq))\n",
    "print(\"FFT estimated season length:\", season_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8bc09",
   "metadata": {},
   "source": [
    "### 📊 FFT Magnitude Spectrum\n",
    "\n",
    "The **FFT magnitude spectrum** reveals strong periodic patterns in the `Appliances` energy usage. A sharp peak appears around the **24-hour period (1 day)**, confirming the dominant **daily consumption cycle**. Additional peaks near 12 h and 6 h suggest possible sub-daily routines (e.g., cooking, heating). The x-axis is shown in **log-scale of time (inverted)**, making it easy to interpret daily and weekly frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute FFT\n",
    "y = df[\"Appliances\"].values\n",
    "y_fft = np.fft.fft(y)\n",
    "frequencies = np.fft.fftfreq(len(y), d=600)  # 10-minute interval = 600s\n",
    "\n",
    "# Positive frequencies only\n",
    "mask = frequencies > 0\n",
    "freqs = frequencies[mask]\n",
    "magnitudes = np.abs(y_fft)[mask]\n",
    "\n",
    "# Convert frequency to period in seconds\n",
    "periods_sec = 1 / freqs\n",
    "periods_hours = periods_sec / 3600\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(periods_hours, magnitudes)\n",
    "plt.title(\"FFT Magnitude Spectrum of Appliance Energy Usage\")\n",
    "plt.xlabel(\"Period (log scale): Minutes, Hours, Days, Months\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.xscale(\"log\")\n",
    "plt.gca().invert_xaxis()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "# Custom ticks (in hours)\n",
    "custom_ticks = [1/6, 1, 6, 12, 24, 24*7, 24*30]  # 10min, 1h, 6h, 12h, 1d, 7d, 30d\n",
    "custom_labels = ['10 min', '1 h', '6 h', '12 h', '1 d', '7 d', '30 d']\n",
    "\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6ca62",
   "metadata": {},
   "source": [
    "### 📈 Seasonal Decomposition (Additive Model)\n",
    "\n",
    "Using a decomposition with **daily periodicity (season_length = 144)**, the `Appliances` signal is broken into:\n",
    "\n",
    "- **Trend:** Captures the longer-term variation in energy usage across weeks.\n",
    "- **Seasonal:** Shows the **strong and regular daily oscillation**, matching the 24-hour cycle.\n",
    "- **Residual:** Contains noise and irregular patterns not explained by trend or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "season_length = 144\n",
    "decomp = seasonal_decompose(df[\"Appliances\"], period=season_length, model='additive')  # adjust `season_length`\n",
    "decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2eac0",
   "metadata": {},
   "source": [
    "# Time Series Regression Task: Forecasting Appliance Energy Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147ecaa",
   "metadata": {},
   "source": [
    "### 📐 Problem Definition\n",
    "\n",
    "The goal of this regression task is to **predict future appliance energy consumption** based on past multivariate environmental and contextual features. The target variable is the normalized energy usage (`Appliances`) recorded every 10 minutes.\n",
    "\n",
    "To frame this as a supervised learning problem, we used a **sliding-window approach** implemented via a custom `SeqDataset`. Each training sample consists of:\n",
    "- An **input window** (`input_len = 5 days`, i.e., 720 timesteps) of past observations (`X` features)\n",
    "- A **forecasting horizon** (`horizon = 100 steps`) representing the future values of the target `y`\n",
    "\n",
    "A key parameter in this construction is the **`stride`**, which controls the step size between consecutive samples:\n",
    "- A stride of `72` (equivalent to 12 hours) reduces overlap and balances dataset size vs. temporal coverage\n",
    "\n",
    "In addition to the continuous variables, **categorical time-based features** such as `hour`, `dayofweek`, and `is_weekend` were included in the regression. These features improved performance, highlighting the relevance of temporal context in forecasting appliance energy usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Data Normalization and Splitting\n",
    "\n",
    "Before creating the sliding sequences, the data is **standardized**:\n",
    "- A `StandardScaler` is fit **only on the training portion** (first 80% of the time series)\n",
    "- Both features (`X`) and target (`y`) are scaled using this strategy to prevent data leakage\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Train/Test Split Strategy\n",
    "\n",
    "To preserve the temporal structure, the time series is not shuffled. Instead:\n",
    "- The dataset is split so that the final 20% of the samples (i.e., the most recent observations) are reserved exclusively for testing\n",
    "- This mimics a realistic forecasting setup where the model predicts the future based only on the past\n",
    "\n",
    "This strategy ensures that all test samples are temporally after the training samples, maintaining causality and simulating real-world inference.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 Model Selection\n",
    "\n",
    "We trained and compared the following three deep learning architectures, suitable for sequence-to-sequence regression:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory):** Captures long-term dependencies in sequential data.\n",
    "- **Transformer:** Attention-based model capable of modeling global temporal dependencies.\n",
    "- **TCN (Temporal Convolutional Network):** Uses causal convolutions and dilation to model temporal structure efficiently.\n",
    "\n",
    "Each model was trained using the same pipeline: 200 epochs, MAE loss, Adam optimizer, and identical dataloaders.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 KPI Selection\n",
    "\n",
    "To evaluate performance on the test set, we computed two standard regression metrics:\n",
    "\n",
    "- **MAE (Mean Absolute Error):** Measures average magnitude of errors (less sensitive to outliers).\n",
    "- **RMSE (Root Mean Squared Error):** Penalizes large errors more heavily, emphasizing precision.\n",
    "\n",
    "These KPIs are shown in a summary table to enable **quantitative comparison between models**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Results\n",
    "\n",
    "| Model        | MAE     | RMSE    |\n",
    "|--------------|---------|---------|\n",
    "| **LSTM**      | 0.472   | 0.889  |\n",
    "| **Transformer** | **0.456** | **0.882** |\n",
    "| **TCN**       | 0.537   | 0.905   |\n",
    "\n",
    "Comparison of models:\n",
    "                   MAE      RMSE\n",
    "lstm         0.472583  0.888938\n",
    "transformer  0.455955  0.881808\n",
    "tcn          0.536792  0.904915\n",
    "\n",
    "From the results:\n",
    "- The **Transformer** model performs best in both MAE and RMSE, indicating better generalization to future unseen windows.\n",
    "- LSTM and TCN also perform reasonably well, with slightly higher error margins.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Visualizing Predictions\n",
    "\n",
    "In the final cell, we selected a test sample to visualize predictions over time. The plot compares the ground truth energy usage with the predictions from each model over the 100-step forecasting horizon.\n",
    "\n",
    "This visual validation helps assess model behavior beyond numeric KPIs, revealing how well the models follow trends, anticipate peaks, or lag behind. The models generally capture the overall energy consumption trend, but they tend to struggle with predicting sudden spikes in usage. Since we are evaluating the energy consumption of a single household, these random increases are difficult to forecast—possibly due to a lack of sufficient information in the dataset to anticipate such events.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Training & Evaluation Workflow\n",
    "\n",
    "The full pipeline follows this modular flowchart:\n",
    "\n",
    "Train model → Save model → Load model → Evaluate model\n",
    "\n",
    "This design supports **reproducibility and modular execution**:\n",
    "- If the models have already been trained and saved to disk, you can **skip the training cell** and directly load and evaluate models using the stored `.pth` weights.\n",
    "- Evaluation is decoupled from training, making it easy to test additional metrics or visualize different test samples.\n",
    "\n",
    "---\n",
    "\n",
    "This workflow provides a flexible, scalable setup for deep time series forecasting and makes it easy to experiment with new models or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6948cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "# Device configuration for macOS Metal or CUDA\n",
    "if torch.backends.mps.is_available() and platform.system() == \"Darwin\":\n",
    "    device = torch.device(\"mps\")  # Apple Silicon (Metal Performance Shaders)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Fallback to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16492606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df['Appliances']                      \n",
    "X = df.drop(columns=['Appliances'])       \n",
    "\n",
    "# 1.4. Normalización (fit solo en train)\n",
    "train_size = int(len(y)*0.8)\n",
    "scaler_X = StandardScaler().fit(X.iloc[:train_size])\n",
    "scaler_y = StandardScaler().fit(y.iloc[:train_size].values.reshape(-1,1))\n",
    "Xs = scaler_X.transform(X)\n",
    "ys = scaler_y.transform(y.values.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "horizon = 100 #144 100\n",
    "input_len = 144*5\n",
    "stride = 72\n",
    "\n",
    "from models.seq_dataloader import SeqDataset\n",
    "\n",
    "# Crear splits\n",
    "ds = SeqDataset(Xs, ys, input_len=input_len, horizon=horizon)\n",
    "n_train = int(len(ds)*0.8)\n",
    "train_ds, test_ds = torch.utils.data.random_split(ds, [n_train, len(ds)-n_train])\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a463e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure models directory exists\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'lstm': LSTMForecaster(n_feats=Xs.shape[1], horizon=horizon).to(device),\n",
    "    'transformer': TransfForecaster(n_feats=Xs.shape[1], horizon=horizon).to(device),\n",
    "    'tcn': TCNForecaster(n_feats=Xs.shape[1], horizon=horizon).to(device)\n",
    "}\n",
    "\n",
    "# Map model names to classes\n",
    "model_classes = {\n",
    "    'lstm': LSTMForecaster,\n",
    "    'transformer': TransfForecaster,\n",
    "    'tcn': TCNForecaster\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "histories = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    hist = fit_model(model, train_loader, test_loader, device, epochs=200, lr=1e-3)\n",
    "    histories[name] = hist\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), f'../models/{name}.pth')\n",
    "    \n",
    "    # Plot and save training loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist['train_loss'], label='train_loss')\n",
    "    plt.title(f\"{name.upper()} Training Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../models/{name}_train_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"Training complete. Models and plots saved in 'models/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28005482",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, cls in model_classes.items():\n",
    "    print(f\"Loading and evaluating {name}...\")\n",
    "    # Re-instantiate model and load weights\n",
    "    model = cls(n_feats=Xs.shape[1], horizon=horizon).to(device)\n",
    "    model.load_state_dict(torch.load(f'../models/{name}.pth'))\n",
    "\n",
    "    # Evaluate\n",
    "    met = eval_model(model, test_loader, device)\n",
    "    results[name] = met\n",
    "    print(f\"{name.upper()} -> MAE: {met['MAE']:.4f}, RMSE: {met['RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff520b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "summary = pd.DataFrame({\n",
    "    name: { 'MAE': met['MAE'], 'RMSE': met['RMSE'] }\n",
    "    for name, met in results.items()\n",
    "}).T\n",
    "print(\"\\nComparison of models:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI summary\n",
    "df_kpi = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'Transformer', 'TCN'],\n",
    "    'MAE':   [results['lstm']['MAE'], results['transformer']['MAE'], results['tcn']['MAE']],\n",
    "    'RMSE':  [results['lstm']['RMSE'], results['transformer']['RMSE'], results['tcn']['RMSE']],\n",
    "})\n",
    "\n",
    "print(df_kpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14949744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT THE TEST SAMPLE TO ANALYSE\n",
    "TEST_SAMPLES = [0, 1, 2, 5, 10]  # you can choose any indices\n",
    "\n",
    "# Plot predictions for each selected test sample\n",
    "for TEST_SAMPLE in TEST_SAMPLES:\n",
    "    start_idx = TEST_SAMPLE * stride + input_len\n",
    "    time_range = df.index[start_idx : start_idx + horizon]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(time_range, results['lstm']['y_true'][TEST_SAMPLE], label='Ground Truth', linewidth=2, color='black')\n",
    "    plt.plot(time_range, results['lstm']['y_pred'][TEST_SAMPLE], '--', label='LSTM', linewidth=1.5)\n",
    "    plt.plot(time_range, results['transformer']['y_pred'][TEST_SAMPLE], ':', label='Transformer', linewidth=1.5)\n",
    "    plt.plot(time_range, results['tcn']['y_pred'][TEST_SAMPLE], '-.', label='TCN', linewidth=1.5)\n",
    "\n",
    "    plt.title(f\"Prediction vs Ground Truth — Test Sample {TEST_SAMPLE}\", fontsize=14)\n",
    "    plt.xlabel(\"Time Index\", fontsize=12)\n",
    "    plt.ylabel(\"Normalized Energy Consumption\", fontsize=12)\n",
    "\n",
    "    # Reduce x-tick clutter\n",
    "    step = 24  # every 4 hours\n",
    "    tick_positions = time_range[::step]\n",
    "    tick_labels = tick_positions.astype(str)\n",
    "    plt.xticks(ticks=tick_positions, labels=tick_labels, rotation=0)\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dada01",
   "metadata": {},
   "source": [
    "# 🚀 Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c3921",
   "metadata": {},
   "source": [
    "\n",
    "This project establishes a solid baseline for time series forecasting using deep learning models. However, several opportunities remain to deepen the analysis and improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Perform a more comprehensive **correlation analysis** between external variables and target consumption to assess feature importance.\n",
    "- Investigate **seasonal, weekly, and hourly patterns** to guide feature engineering and model design.\n",
    "- Visualize **anomalies, outliers, or structural breaks** in the data that may require preprocessing or specialized modeling.\n",
    "- Apply **dimensionality reduction techniques** (e.g., PCA, UMAP) to assess redundancy among input features.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 Classical Machine Learning Models\n",
    "\n",
    "- Integrate traditional statistical forecasting models for comparison:\n",
    "  - **ARIMA**, **SARIMA**, and **Exponential Smoothing**\n",
    "  - Models that explicitly capture seasonality and trend\n",
    "- Implement **hybrid models** combining ARIMA for trend and DL for residual patterns.\n",
    "- Evaluate the effect of **lag feature engineering** for tree-based regressors (e.g., XGBoost, LightGBM).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Deep Learning Optimization\n",
    "\n",
    "- **Hyperparameter tuning:**\n",
    "  - Apply **grid search** or **Bayesian optimization** to find optimal learning rates, dropout rates, layer sizes, etc.\n",
    "- **Architectural improvements:**\n",
    "  - Experiment with **deeper networks**, **attention mechanisms**, or **multi-resolution inputs**\n",
    "- **Training strategies:**\n",
    "  - Use **learning rate schedulers**, **early stopping**, and **batch normalization**\n",
    "  - Experiment with **different loss functions** (e.g., Huber, quantile loss)\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Forecasting Horizon and Input Window\n",
    "\n",
    "- Investigate the impact of **longer input windows** (e.g., 7–10 days) to improve long-term forecasting.\n",
    "- Analyze the trade-off between **input length and model complexity** to avoid overfitting.\n",
    "- Consider **multi-resolution input sequences** (e.g., daily + hourly patterns) for richer temporal context.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Evaluation and Validation\n",
    "\n",
    "- Add **cross-validation over time** (e.g., rolling origin) to assess robustness.\n",
    "- Monitor **temporal generalization** across seasons or time-of-day segments.\n",
    "- Visualize **prediction intervals** or **uncertainty estimation** using quantile regression or dropout-based methods.\n",
    "\n",
    "---\n",
    "\n",
    "These directions aim to build a more robust, interpretable, and scalable forecasting pipeline suitable for deployment or further research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-testenv-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
